## Lista de passos que considero essenciais para a conclusão do desafio

- [x] Como medir a performance em golang? 
    - Aprendi a gerar os benchmarks e analisar os principais indicadores de performance (detalhes sobre os comandos em LEARNING.md)
    - Deixei o exemplo prático utilizado na pasta steps/1-performance (O exemplo serviu para o próximo passo)
- [x] Qual a estratégia mais rápida para normalização das strings?
    - De/para com um range fixado de caracteres conhecidos (pt-BR)
    - Decodificação híbrida para diferenciar caracteres ASCII dos demais.
    - O exemplo prático do melhor resultado que obtive (até agora rss) está em steps/2-normalize
        - 9120418 operações em 2.563s, 131.3 ns/op
- [x] Como ler grandes arquivos em buffer e fazer o parse corretamente?
    - Utilizei o buffio, disponível da própria linguagem
        - Encontrei uma melhoria na porformance setando manualmente o tamanho de buffer de leitura
    - O exemplo prático com o melhor resultado que obtive está em steps/3-parsing
        - 164 operações em 2.052s, 6934582 ns/op
- [x] Como enviar inserts em batch para o banco?
    - [x] Definir estrutura da tabela e como criá-la no banco
        - Não utilizei valores fixos para o tamanho dos campos (ex varchar(50)), por dois motivos:
            - A decisão do tamanho apenas pela amostragem da base fictícia pode trazer problemas futuros em um caso real
            - A diferença de performance é mínima, mas é cabível caso exista um contrato definido para este arquivo
        - O uso de índices é crucial para otimizar as consultas em tabelas realmente grandes
            - Apesar comprometer um pouco a performance do insert, deixei um exemplo como prova de conhecimento
        - O exemplo prático com o melhor resultado que obtive está em steps/4-insert (para 50k registros)
            - 2 operações em 2.046s, 583320104 ns/op
- [x] Copyfrom ou batch insert com paralelismo, qual é mais rápido?
    - É possível quebrar os batchs em partes menores para evitar sobrecarga de memória
        - Isso permite o processamento de arquivos massivos, testei com 5 milhões de registros sem consumir muita 
    - Utilizei uma técnica chamada processamento em fluxo com paralelismo (go routines)
        - Criei um canal de dados e um produtor de dados para alimentar este canal
        - Os trabalhadores ouvem a fila do canal de dados e executam em paralelo de acordo com o número de cores e tamanho da fila
            - Fiz vários testes utilizando uma carga de 5 milhões de registros
                - Caso exista apenas 1 core, tudo funciona normalmente mas é processado de maneira sequencial
                - O tempo de execução se mostrou muito satisfatório com 4 cores (workers) e uma fila (chunk) de 5 mil registros
                    - Sem mudanças muito significativas ao aumentar o número de cores e o tamanho da da fila
                        - Resultados para 50 mil registros
                        - CopyFrom 183.015583ms em steps/5-batch-insert-multicore (streaming, menos memória e melhor cenário)
                        - Batch Insert 312.542583ms em steps/6-copy-from-multicore
- [x] Criar uma solução com tudo o que foi testado
    - Utilizei os melhores resultados de cada etapa
    - Separei a estrutura de arquivos por responsabilidade na raiz do projeto
    - Atingi o objetivo em 100ms, ambiente local e utilizando os 8 cores (Macbook Air M1)
- [x] Dimensionar o banco de dados e a aplicação
    - Rodei um arquivo com 5 milhões de registros (600MB) e apareceu um alerta do postgres sugerindo o aumento do max_wal_size
        - O WAL é uma espécie de diário de transações, que ocupa espaço em disco antes de fazer uma alteração permanente nos dados
            - A velocidade das inserções em lote fizeram o tamanho do WAL crescer até chegar ao máximo e provocar um checkpoint
                - O checkpoint interrompe as operações de I/O por um instante para garantir que as mudanças sejam aplicadas
        - É importante investigar e dimensionar corretamente toda a solução de acordo com os recursos disponíveis
            - Isso não é necessário para o desafio proposto (40k registros)
- [x] Docker e documentação
    